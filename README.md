# Tiverton-Advisors-Case-Study
Question 2 from case study for Data Engineering Analyst Role

This analysis started with an overview of the data in Data Wrangler, an extension in VS Code that I've found very useful when attempting to quickly glean surface-level insights from a new dataset. From there, I broke down into deeper EDA and data correction/cleaning, creating some graphs illustrating some assumptions that I made about the distribution of the data.

I used DuckDB to write the SQL queries for this assignment, pulling from the cleaned dataset saved as "data_cleaned" within the Python environment when the user runs all chunks in this notebook.

A memo to an investment analyst is present at the bottom of this notebook, including some summarized insights from our SQL queries and notes about data cleaning procedures completed (and not completed).

Note about AI usage: I integrate Claude Code into my VS Code instance. I didn't use explicit Claude prompts for this assignment - instead, I used comments and the auto-complete function/code suggestions to generate output closer to what I wanted to create more rapidly. I decided to do this to give more insight into my thought process rather than attempting to prompt engineer a high volume of output I didn't fully understand.
